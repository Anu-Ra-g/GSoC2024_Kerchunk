{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast NODD GRIB Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial we are going to demonstrate building kerchunk aggregations of **NODD grib2 weather forecasts** fast. This workflow primarily involves [xarray-datatree](https://xarray-datatree.readthedocs.io/en/latest/), [pandas](https://pandas.pydata.org/) and `grib_tree` function released in **kerchunkv0.2.3** for the operation.\n",
    "\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "For this operation we will be looking at GRIB2 files generated by [**NOAA Global Ensemble Forecast System (GEFS)**](https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast), is a weather forecast model made up of 21 separate forecasts, or ensemble members. With global coverage, GEFS is produced four times a day with weather forecasts going out to 16 days, with an update frequency of 4 times a day, every 6 hours starting at midnight.\n",
    "\n",
    "More information on this dataset can be found [here](https://registry.opendata.aws/noaa-gefs)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Pandas Tutorial](https://foundations.projectpythia.org/core/pandas/pandas.html#) | Required | Core |\n",
    "| [Kerchunk and Xarray-Datatree](https://projectpythia.org/kerchunk-cookbook/notebooks/using_references/Datatree.html) | Required | IO |\n",
    "| [Xarray-Datatree Overview](https://xarray-datatree.readthedocs.io/en/latest/quick-overview.html)| Required | IO |\n",
    "\n",
    "- **Time to learn**: 30 minutes\n",
    "\n",
    "## Motivation\n",
    "\n",
    "As we know that **kerchunk**  provides a unified way to represent a variety of chunked, compressed data formats (e.g. NetCDF/HDF5, GRIB2, TIFF, â€¦) by generating *references*. This task flow has ability to build large aggregations from **NODD grib forecasts**\n",
    "in a fraction of the time using the `idx files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerchunk.grib2 import (\n",
    "    scan_grib,\n",
    "    grib_tree, \n",
    "    parse_grib_idx, \n",
    "    extract_datatree_chunk_index, \n",
    "    strip_datavar_chunks,\n",
    "    build_idx_grib_mapping, \n",
    "    map_from_index, \n",
    "    reinflate_grib_store,\n",
    "    AggregationType\n",
    ")\n",
    "import copy\n",
    "import pandas as pd\n",
    "import datatree\n",
    "import fsspec\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Aggregation directly from the GRIB files\n",
    "\n",
    "For building the aggregation, we're going to build a hierarchical data model from a set of scanned grib messages with the help of `grib_tree` function. This data model can be opened directly using either zarr or xarray datatree. Here we're going to use `xarray-datatree` to open and view it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_files = [\n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af006\", \n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af012\", \n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af018\",\n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af024\",\n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af030\",\n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af036\",\n",
    "    \"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af042\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the references into the hierarchical datamodel\n",
    "grib_tree_store = grib_tree([group for f in s3_files for group in scan_grib(f, storage_options=dict(anon=True))], remote_options=dict(anon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the output to datatree to view it. This tree model the variables\n",
    "s3_dt = datatree.open_datatree(fsspec.filesystem(\"reference\", fo=grib_tree_store, remote_protocol=\"s3\", remote_options={\"anon\": True}).get_mapper(\"\"), engine=\"zarr\", consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tree model, the variables are organized into hierarchical groups, first by \"stepType\" and then by \"typeOfLevel.\"\n",
    "s3_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: This method is extremely slow if grib files are large or while building the aggregation for a large number of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the aggregation faster with `idx` files\n",
    "\n",
    "### Index Dataframe made from a single Grib file\n",
    "\n",
    "The purpose of using the `idx` files in the aggregation is that the k(erchunk) index data looks a lot like the idx files that are present for every grib file in NODD's **GCS** and **AWS** archive though. \n",
    "\n",
    "This way of building of aggregation only works for a particular `horizon` file irrespective of the run time of the model. \n",
    "\n",
    "Here is what the contents of an `idx` file looks like.\n",
    "\n",
    "```\n",
    "1:0:d=2017010100:HGT:10 mb:12 hour fcst:ENS=low-res ctl\n",
    "2:48163:d=2017010100:TMP:10 mb:12 hour fcst:ENS=low-res ctl\n",
    "3:68112:d=2017010100:RH:10 mb:12 hour fcst:ENS=low-res ctl\n",
    "4:79092:d=2017010100:UGRD:10 mb:12 hour fcst:ENS=low-res ctl\n",
    "5:102125:d=2017010100:VGRD:10 mb:12 hour fcst:ENS=low-res ctl\n",
    "6:122799:d=2017010100:HGT:50 mb:12 hour fcst:ENS=low-res ctl\n",
    "7:178898:d=2017010100:TMP:50 mb:12 hour fcst:ENS=low-res ctl\n",
    "8:201799:d=2017010100:RH:50 mb:12 hour fcst:ENS=low-res ctl\n",
    "9:224321:d=2017010100:UGRD:50 mb:12 hour fcst:ENS=low-res ctl\n",
    "10:272234:d=2017010100:VGRD:50 mb:12 hour fcst:ENS=low-res ctl\n",
    "11:318288:d=2017010100:HGT:100 mb:12 hour fcst:ENS=low-res ctl\n",
    "12:379010:d=2017010100:TMP:100 mb:12 hour fcst:ENS=low-res ctl\n",
    "13:405537:d=2017010100:RH:100 mb:12 hour fcst:ENS=low-res ctl\n",
    "14:441517:d=2017010100:UGRD:100 mb:12 hour fcst:ENS=low-res ctl\n",
    "15:497421:d=2017010100:VGRD:100 mb:12 hour fcst:ENS=low-res ctl\n",
    "```\n",
    "\n",
    "The general format of `idx` data across the **NODD** cloud platforms is: `index:offset:date_with_runtime:variable:forecast_time:`.<br>\n",
    "The metadata are separated by \":\" (colon) and we need to convert it into a `Dataframe` for the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the idx data into a dataframe\n",
    "idxdf = parse_grib_idx(\"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af006\", storage_options=dict(anon=True))\n",
    "idxdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a mapping between the index dataframe and grib metadata\n",
    "\n",
    "Now we're going to need a mapping from our grib/zarr metadata to the attributes in the idx files. They are unique for each time horizon e.g. you need to build a unique mapping for the 1 hour forecast, the 2 hour forecast and so on. So in this step we're going to create a **mapping** for a single grib file and its corresponding `idx` files in order, which will be used in later steps for building the aggregation. \n",
    "\n",
    "Before that let's see what **grib data** we're extracting from the datatree. The metadata that we'll be extracting will be static in nature. We're going to use a single node by [accessing](https://projectpythia.org/kerchunk-cookbook/notebooks/using_references/Datatree.html#accessing-the-datatree) it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the grib metadata from a single datatree node and converting it into a dataframe\n",
    "grib_df = extract_datatree_chunk_index(s3_dt[\"ulwrf/avg/nominalTop\"], grib_tree_store)\n",
    "grib_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Above process is part of the mapping creation, the function call to `extract_datatree_chunk_index` in handled inside `build_idx_mapping` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mapping for a single horizon file which is to be used later\n",
    "mapping = build_idx_grib_mapping(\"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af006\", storage_options=dict(anon=True), remote_options=dict(anon=True), validate=True)\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the index \n",
    "\n",
    "Now if we parse the runTime from the idx file , we can build a fully compatible k_index(kerchunk index).\n",
    "Before creating the index, we need to clean some of the data in the mapping and index dataframe for the some variables as they tend to contain duplicate values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step will be performed for every grib-idx pair where we will be using the \"mapping\" dataframe which we created previously \n",
    "mapped_index = map_from_index(\n",
    "    pd.Timestamp(\"2017-01-01T06\"),\n",
    "    mapping.loc[~mapping[\"attrs\"].duplicated(keep=\"first\"), :],\n",
    "    idxdf.loc[~idxdf[\"attrs\"].duplicated(keep=\"first\"), :],\n",
    ")\n",
    "mapped_index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step of building of Aggregation\n",
    "\n",
    "Here we're going to build the aggregation from the date `2017-01-01` to `2017-02-28` for the `6-hour` horizon file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_index_list = []\n",
    "\n",
    "deduped_mapping = mapping.loc[~mapping[\"attrs\"].duplicated(keep=\"first\"), :]\n",
    "\n",
    "for date in pd.date_range(\"2017-01-01\", \"2017-02-28\"):\n",
    "  for runtime in range(0, 24, 6):\n",
    "    fname = f\"s3://noaa-gefs-pds/gefs.{date.strftime('%Y%m%d')}/{runtime:02}/gec00.t{runtime:02}z.pgrb2af006\"\n",
    "    \n",
    "  idxdf = parse_grib_idx(basename=fname, storage_options=dict(anon=True))\n",
    "\n",
    "  mapped_index = map_from_index(\n",
    "      pd.Timestamp(date + datetime.timedelta(hours=runtime)),\n",
    "      deduped_mapping,\n",
    "      idxdf.loc[~idxdf[\"attrs\"].duplicated(keep=\"first\"), :],\n",
    "  )\n",
    "\n",
    "  mapped_index_list.append(mapped_index)\n",
    "\n",
    "s3_kind = pd.concat(mapped_index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [\n",
    "  pd.Index(\n",
    "    [\n",
    "      pd.timedelta_range(start=\"0 hours\", end=\"24 hours\", freq=\"6h\", closed=\"right\", name=\"6 hour\"),\n",
    "    ],\n",
    "    name=\"step\"\n",
    "  ),\n",
    "  pd.date_range(\"2017-01-01T00:00\", \"2017-03-01T00:00\", freq=\"360min\", name=\"valid_time\")\n",
    "]\n",
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_tree_store = grib_tree(scan_grib(\"s3://noaa-gefs-pds/gefs.20170101/06/gec00.t06z.pgrb2af006\", storage_options=dict(anon=True)), remote_options=dict(anon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinflating works with small part of the grib tree and separating out the static metadata\n",
    "\n",
    "deflated_s3_grib_tree_store = copy.deepcopy(grib_tree_store)\n",
    "strip_datavar_chunks(deflated_s3_grib_tree_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_store = reinflate_grib_store(\n",
    "    axes=axes,\n",
    "    aggregation_type=AggregationType.HORIZON,\n",
    "    chunk_index=s3_kind.loc[s3_kind.varname.isin([\"ulwrf\", \"prmsl\"])],\n",
    "    zarr_ref_store=deflated_s3_grib_tree_store  #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_dt_subset = datatree.open_datatree(fsspec.filesystem(\"reference\", fo=s3_store, remote_protocol=\"s3\", remote_options={\"anon\": True}).get_mapper(\"\"), engine=\"zarr\", consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_dt_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfBoundsTimedelta",
     "evalue": "Cannot convert 4618441417868443648 hours to timedelta64[ns] without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32mnp_datetime.pyx:436\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.np_datetime.astype_overflowsafe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnp_datetime.pyx:252\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.np_datetime.check_dts_bounds\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m: Out of bounds nanosecond timestamp: 526869296641467-12-02 16:00:00",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutOfBoundsTimedelta\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ms3_dt_subset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mulwrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnominalTop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mulwrf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/plot/accessor.py:48\u001b[0m, in \u001b[0;36mDataArrayPlotAccessor.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(dataarray_plot\u001b[38;5;241m.\u001b[39mplot, assigned\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataarray_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_da\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/plot/dataarray_plot.py:270\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(darray, row, col, col_wrap, ax, hue, subplot_kws, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m    220\u001b[0m     darray: DataArray,\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Default plot of DataArray using :py:mod:`matplotlib:matplotlib.pyplot`.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    xarray.DataArray.squeeze\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     darray \u001b[38;5;241m=\u001b[39m \u001b[43mdarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msizes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     plot_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(darray\u001b[38;5;241m.\u001b[39mdims)\n\u001b[1;32m    273\u001b[0m     plot_dims\u001b[38;5;241m.\u001b[39mdiscard(row)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/dataarray.py:1194\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/dataarray.py:1162\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1162\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/dataset.py:875\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lazy_data:\n\u001b[0;32m--> 875\u001b[0m         \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/variable.py:977\u001b[0m, in \u001b[0;36mVariable.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    961\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this variable's data from disk or a\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this variable.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m    dask.array.compute\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/namedarray/pycompat.py:134\u001b[0m, in \u001b[0;36mto_duck_array\u001b[0;34m(data, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loaded_data\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExplicitlyIndexed):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-untyped-call, no-any-return]\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_duck_array(data):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/indexing.py:837\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/indexing.py:831\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/indexing.py:788\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/core/indexing.py:658\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n\u001b[0;32m--> 658\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_numpy_scalars(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/coding/variables.py:81\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/coding/times.py:374\u001b[0m, in \u001b[0;36mdecode_cf_timedelta\u001b[0;34m(num_timedeltas, units)\u001b[0m\n\u001b[1;32m    372\u001b[0m num_timedeltas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(num_timedeltas)\n\u001b[1;32m    373\u001b[0m units \u001b[38;5;241m=\u001b[39m _netcdf_to_numpy_timeunit(units)\n\u001b[0;32m--> 374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mto_timedelta_unboxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_timedeltas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reshape(result, num_timedeltas\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/xarray/coding/times.py:357\u001b[0m, in \u001b[0;36mto_timedelta_unboxed\u001b[0;34m(value, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_timedelta_unboxed\u001b[39m(value, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 357\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_timedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimedelta64[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/pandas/core/tools/timedeltas.py:213\u001b[0m, in \u001b[0;36mto_timedelta\u001b[0;34m(arg, unit, errors)\u001b[0m\n\u001b[1;32m    211\u001b[0m     arg \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(arg)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, timedelta, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/pandas/core/tools/timedeltas.py:266\u001b[0m, in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, unit, errors, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     td64arr \u001b[38;5;241m=\u001b[39m \u001b[43msequence_to_td64ns\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/pandas/core/arrays/timedeltas.py:1047\u001b[0m, in \u001b[0;36msequence_to_td64ns\u001b[0;34m(data, copy, unit, errors)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# treat as multiples of the given unit\u001b[39;00m\n\u001b[0;32m-> 1047\u001b[0m     data, copy_made \u001b[38;5;241m=\u001b[39m \u001b[43m_ints_to_td64ns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy_made\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_float_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# cast the unit, multiply base/frac separately\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# to avoid precision issues from float -> int\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/pandas/core/arrays/timedeltas.py:1115\u001b[0m, in \u001b[0;36m_ints_to_td64ns\u001b[0;34m(data, unit)\u001b[0m\n\u001b[1;32m   1112\u001b[0m dtype_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimedelta64[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(dtype_str)\n\u001b[0;32m-> 1115\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mastype_overflowsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTD64NS_DTYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# the astype conversion makes a copy, so we can avoid re-copying later\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m copy_made \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32mnp_datetime.pyx:358\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.np_datetime.astype_overflowsafe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnp_datetime.pyx:447\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.np_datetime.astype_overflowsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsTimedelta\u001b[0m: Cannot convert 4618441417868443648 hours to timedelta64[ns] without overflow"
     ]
    }
   ],
   "source": [
    "s3_dt_subset.ulwrf.avg.nominalTop.ulwrf[0,0,0:10,8].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
